# Overview
- This document covers the implementation of L-BFGR methods for QSP protocol optimization as covered in [this paper](https://arxiv.org/pdf/2002.11649) and [this package subroutine](https://github.com/qsppack/QSPPACK/blob/master/Solvers/Optimization/QSP_solver.m)

## Notes on some basic functionality in original package
- For response functions
	- Contained in `response.py`, and simply loops over input data and evaluates the proper matrix product.
- Classes exist for polynomial types in `LPoly.py`, with personal properties and class methods
- Special classes of polynomials returned by `poly.py`; this is useful for tests
- For tests:
	- Import `unittest`, `numpy`, and `pyqsp` with derived subfiles often for some reason.
	- Note testing run per `pytest pyqsp/test/test_sym_qsp_optimization.py`, or `python -m unittest pyqsp/test/test_sym_qsp_optimization.py`

## Notes on remaining items to match QSPPack implementation
- [ ] Currently parity is automatically determined from the full phases; generate two methods for bidirectional conversion with an optional parity parameter.
- [ ] Direct plotting GUI is helpful for debugging; eventually this can be converted to something visible for, e.g., bandpass functions.
	- The GUI could be for common functions, and if fast enough, can directly plot and list the phases. Values can be entered, and absolute error can be plotted on a separate graph below with modified axes.
	- This can also automatically generate the command to run with the relevant parameters, allowing for someone to copy paste into their code
- [ ] Current polynomial generators in the `poly.py` file should have the option to directly return Chebyshev coefficients, as this might be a bottleneck for current plotting.
- [ ] Try to port method for computing bandpass function from other repos, as a supplement to the simpler optimization schemes currently implemented here.
- [ ] Determine how best to integrate current new module with the command line interface allowed by previous repo
- [ ] `cheb2poly` method is currently used in `poly.py` in three locations: `PolyCosineTX`, `PolySineTX`, `PolyOneOverX`.
- [ ] We might need to implement something closer to `https://github.com/qsppack/QSPPACK/blob/master/Solvers/Optimization/cvx_poly_coef.m` to achieve phase sequences of the proper length.

## Required components
- Overall QSP solver file
	- Input parameters in original `QSP_solver.m`
		- Max iterations
		- Stop criteria
		- Whether to use real arithmetic or not
		- Whether to use `Pre` as target function
		- Method selection (for us, we will always use LBFGS)
		- Whether to use full or reduced phase factors
	- Output parameters in original `QSP_solver.m`
		- Full phase factors
		- Other metadata
- Order of current computation
	- For within LBFGS
		- Given total length of coefficients, compute `delta`]
		- Set target, depending on `Pre` flat (*currently not understood*), to a returned value computed by `ChebyCoef2Func`, which takes in x, coefficients, and parity arguments. Note this creates an anonymous function depending on (x, opts), where opts is the metadata discussed above.
		- Generates a series of objects
			- obj is set to `@QSPObj_sym`
			- grad is set to `@QSPGrad_sym` or `QSPGrad_sym_real` depending on real flag
		- *MAIN FUNCTION CALL*
			- The LFBGS solver module is called, which returns `phi`, `err`, and `iter` data, and takes in `obj` and `grad` from above, as well as `delta` and metadata
			- *NOTE*: depending on parity, initial phase is halved
		- Phases are returned, possibly calling `rdc_phase_factor_to_full` if they were reduced
			- *QUESTION*: if we work only with non-reduced factors, is it difficult to ensure the proper optimization?

## Notes on `QSP_LBFGS.m` file, called as main subroutine
- Overall structure
	- Input
		- `obj`: objective function L(phi); generated by `@QSPObj_sym`.
		- `grad`: gradient of object function.
		- `delta`: number of samples.
		- `phi`: initial value.
		- `opts`: metadata.
			- maxiter, gamma (linesearch retraction rate), accrate (linesearch accept ratio), minstep, criteria (*stop criteria for obj value on Chevyshev points*), lmem (memory size), print (whether to output), itprint, parity ((0 -- even, 1 -- odd)), target (polynomial)
	- Output
		- phi
		- error of final effort according to loss
		- iteration number
	- Set and initialize all metadata as given
	- Call and initial gradient and objects
		- Call `grad(phi,delta,opts)` to get `grad_s` and `obj_s`
		- Compute means of these values (presumably sampled over)
		- Initial `obj_value` and `GRAD` as the first mean, and the second mean's transpose
	- Then we iterate while true
		- Start with `GRAD` and `alpha` (zero list the size of lmem), and then we perform two loops
			- *FIRST LOOP*
				- Looping over lmem size, we update alpha(i) according to some strange indexing scheme dependent theta_d (`GRAD` initially) and all of (initially zero)
					- `mem_dot`: (lmem)
					- `mem_obj`: (lmem, d), for d len(phi)
					- `mem_grad`: (lmem, d)
				- Then depending on parity rescaling first phase again
			- *SECOND LOOP*
				- Now we generate `beta` with 
					- `mem_size` and `mem_now` scalars
				- This depends on `mem_grad`, `mem_dot`, `mem_obj`, and `alpha`, with self updates.
		- We reach another if True loop
			- Computes a new set of phases according to `step` and `theta_d`
			- Computes objective function on new phases
			- Computes mean of those objectives
			- Computes how much change from last iteration `ad`
			- Breaks if step gets too small (decreasing multiplicatively by gamma), or *if too large?*
				- *NOTE*: these look like the Wolfe conditions following from an inexact line search. We require that the slope is reduced sufficiently (we're not stepping too far), and that we're making progress.
		- Then we move onto a final step in the outer overall loop
			- Set as phi the temp theta found in previous step
			- Compute the objective and the *MAX* of those samples
			- Compute a new gradient
			- Update all the `mem_` objects from before by real values
				- `mem_size` is increased by one until limit lmem 
				- `mem_now` is increased by one (and modulo lmem)
				- `mem_grad` (Column `mem_now`) is updated to the difference in gradient before and after
				- `mem_obj` (Column `mem_now`) is taken to be `-step` times`theta_d`
				- `mem_dot` (at index `mem_now`) is taken as the inner product of the `mem_now`-th column of `mem_grad` and `mem_obj` (the latter transposed).
			- The `GRAD` is updated to the new grad computed
		- We end if one of two conditions is met
			- We reach `max_iter`
			- If the worst case `obj_max` objective error computed above is less than `crit` (*SQUARED* for some reason).
	- Note that the structure is covered more clearly on 22 of [this paper](https://arxiv.org/pdf/2002.11649)
	- Note also Appx D proof that Chebyshev nodes are enough for accurate computation.


## On computing gradients
- *Remaining questions*; is this some form of conjugate gradient descent; it's unclear what the gradient estimation algorithm is, and why it involves a forward and backward pass over the phases.
	- Think of this as what happens when we compute the derivative with respect to some middle phase. We get something like sigma z in the center
	- The use of the gap parameter seems to assume that we're close to the ideal value
	- The factor of gap comes directly from applying the chain rule to `0.5*(real(qspmat(1,1))-targetx(x))^2;`, which leads to `2*gap*real(grad_tmp)`. And this temp grad is just computed by hand as one would expect; try to write this out entirely to see how it depends on the relevant product
	- We have actual products and elementwise products here. Note that `.*` together imply elementwise operations.
- Located in `QSPGrad_sym.m` and symmetrized real version
	- *NOTE: THIS SECTION IS UNFINISHED*
	- Computes the gradient with respect to each phase of the objective function, assuming that phi is symmetric.
- Overall program structure
	- Inputs:
		- `phi`: the varying parameters
		- `delta`: samples (*QUESTION: DETERMINE HOW THESE ARE SELECTED*).
			- At the top level module, chosen as `delta = cos((1:2:(2*tot_len-1))*(pi/2/(2*tot_len)))'`; these look like the Chebyshev nodes. The question now is *Why is sampling over this sufficient for computing a good gradient?*
			- *ANSWER*: this is not about sampling or averaging, it's about finding a good pointwise evaluation of the uniform error between the achieved and target functions. 
		- `opts`: various metadata like the target function and parity.
	- Outputs:
		- `grad` vector
		- `obj` evaluation of objective function
	- Critical component structure
		- Loop over all chebyshev nodes defined by delta
			- Initialize `x`, initial W(x), and generate two length d (len(phi)) lists of 2x2 matrices
			- The *first* of these is filled by identity matrices
			- The *second* of these is filled with z-rotations by `exp(1j * phi(d))` mulitiplied with the *S* gate descibed previously, called `gate`
			- Then we loop over the phi's, from 2 to d
				- We fill the `j`-th of these 2x2 matrices with something depending on the previous one. Namely a product of W(x) and the relevant z-rotation according to phi.
				- The second list works backward through the phis, but does the same thing.
			- Then we split into two cases based on parity
				- WLOG, considering the `parity = 1` case
					- We compute `qspmat` which is conjugation by the l*ast element of the second matrix* of the signal operator (presumably recovering the symmetrized form)
					- We compute `gap`, which is the element error at the specified `x`
					- A placeholder matrix is computed `leftmat`
					- Then we loop over all phases *THIS IS THE CRITICAL STEP*
						- We update a `grad_tmp` matrix again in steps, which looks at something like a symplectic product of the `j`-th element of the first and second lists of matrices computed in the previous step.
						- Then the `(i, j)` element of the grad becomes the (1,1) element of the j-th component times the `gap` variable specified above.
						- Finally obj(i) is just `(1/2)*gap**2`.
				- The only difference in the parity case is the addition of `grad(i,1) = grad(i,1)/2` in the `parity = 0` case.
		

## On computing loss function
- The objective function should be contained in the grad computation, but is also called with `QSPObj_sym.m`
	- This performs very simple arithmetic on the top-right element of the unitary computed by `QSPGetUnit_sym.m` (half of the squared error for some reason)
		- *NOTES ON THE FORMAT OF THIS UNITARY*: Both this and `QSPGetUnitary.m` just perform the same repeated product of unitaries.
		- *NOTE FOR SYM WE ARE CONJUGATING BY THE RET GATE, WHICH LOOKS LIKE AN S GATE*. 
		- *NOTE*: target is specified by `ChebyCoef2Func` function at the top level
	- Comparison is made with `opts.target(delta(i))`; namely for each of the samples according to `delta`, the i-th component of this sample list has a function applied to it, and is compared with the real part of the QSP unitary element applied to the same `delta(i)`.

## On `ChebyCoef2Func.m`
- Takes the cheb coefficients and an argument and returns a function at that argument.
- *QUESTION*: unclear what the `Pre` (p real?) condition is at the top level module, which only changes the sign of the objective
- *NOTE*: this is different from `ChebyFunc2Coef`, which gives the chebyshev coefficients for a given polynomial up to a certain order.

## On other methods for optimization and standard phase finding
- Hessian and Jacobian use are restricted to Haah/GSLW method and strict Newton method respectively.

## Design notes
- Symmetric phases are used to achieve a given real part of the (1,1) component of the overall unitary
	- *NOTE*: optimization is often started from a fixed, non-zero initial state.
- Data types and class structure
	- Full or reduced phase factors; we could specify a full phases object, or a QSP object; something which contains, if one desires to query attributes of a protocol, everyone one wants to know. This could be instantiated in one of many ways. *NOTE: look at what the package currently does.*